{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/varunav123/LZ77/blob/master/imagecol.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIb-j3AlyfEU",
        "outputId": "a503de65-78e3-4f69-f0ed-e608395c6709"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjRerFkJ2Dsl",
        "outputId": "4b3cb183-99cf-41be-c80e-3008471d3fca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/MyDrive/archive.zip\n",
            "replace val_256/test_samples/Places365_val_00000006.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!unzip /content/drive/MyDrive/archive.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdQCxnib2GGF"
      },
      "outputs": [],
      "source": [
        "import cv2 as cv\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from matplotlib import image\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "import time\n",
        "\n",
        "from tensorflow import keras\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqt8Pkag-nhV"
      },
      "outputs": [],
      "source": [
        "tf.config.run_functions_eagerly(True)\n",
        "\n",
        "# The batch size we'll use for training\n",
        "batch_size = 64\n",
        "\n",
        "# Size of the image required to train our model\n",
        "img_size = 120\n",
        "\n",
        "# These many images will be used from the data archive\n",
        "dataset_split = 2500\n",
        "\n",
        "master_dir = 'val_256/train_samples'\n",
        "x = []\n",
        "y = []\n",
        "for image_file in os.listdir(master_dir)[:dataset_split]:\n",
        "    rgb_image = cv.imread(os.path.join(master_dir, image_file))\n",
        "    rgb_image = cv.cvtColor(rgb_image, cv.COLOR_BGR2RGB)\n",
        "    rgb_image = cv.cvtColor(rgb_image, cv.COLOR_RGB2LAB)\n",
        "    rgb_image = cv.resize(rgb_image, (img_size, img_size))\n",
        "    rgb_img_array = rgb_image.astype(np.float32) / 255.0\n",
        "\n",
        "\n",
        "    gray_image = cv.cvtColor(rgb_image, cv.COLOR_RGB2GRAY)\n",
        "    gray_image = gray_image.reshape((img_size, img_size, 1))\n",
        "    gray_img_array = gray_image.astype(np.float32) / 255.0\n",
        "\n",
        "    x.append(gray_img_array)\n",
        "    y.append(rgb_img_array)\n",
        "\n",
        "# Train-test splitting\n",
        "train_x, test_x, train_y, test_y = train_test_split(np.array(x), np.array(y), test_size=0.1)\n",
        "\n",
        "# Construct tf.data.Dataset object\n",
        "dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
        "dataset = dataset.batch(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6_6nxRNafU_"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeX6OPXhe1Ao"
      },
      "outputs": [],
      "source": [
        "def get_generator_model():\n",
        "\n",
        "    inputs = tf.keras.layers.Input( shape=( img_size , img_size , 1 ) )\n",
        "\n",
        "    conv1 = tf.keras.layers.Conv2D( 16 , kernel_size=( 5 , 5 ) , strides=1 )( inputs )\n",
        "    conv1 = tf.keras.layers.LeakyReLU()( conv1 )\n",
        "    conv1 = tf.keras.layers.Conv2D( 32 , kernel_size=( 3 , 3 ) , strides=1)( conv1 )\n",
        "    conv1 = tf.keras.layers.LeakyReLU()( conv1 )\n",
        "    conv1 = tf.keras.layers.Conv2D( 32 , kernel_size=( 3 , 3 ) , strides=1)( conv1 )\n",
        "    conv1 = tf.keras.layers.LeakyReLU()( conv1 )\n",
        "\n",
        "    conv2 = tf.keras.layers.Conv2D( 32 , kernel_size=( 5 , 5 ) , strides=1)( conv1 )\n",
        "    conv2 = tf.keras.layers.LeakyReLU()( conv2 )\n",
        "    conv2 = tf.keras.layers.Conv2D( 64 , kernel_size=( 3 , 3 ) , strides=1 )( conv2 )\n",
        "    conv2 = tf.keras.layers.LeakyReLU()( conv2 )\n",
        "    conv2 = tf.keras.layers.Conv2D( 64 , kernel_size=( 3 , 3 ) , strides=1 )( conv2 )\n",
        "    conv2 = tf.keras.layers.LeakyReLU()( conv2 )\n",
        "\n",
        "    conv3 = tf.keras.layers.Conv2D( 64 , kernel_size=( 5 , 5 ) , strides=1 )( conv2 )\n",
        "    conv3 = tf.keras.layers.LeakyReLU()( conv3 )\n",
        "    conv3 = tf.keras.layers.Conv2D( 128 , kernel_size=( 3 , 3 ) , strides=1 )( conv3 )\n",
        "    conv3 = tf.keras.layers.LeakyReLU()( conv3 )\n",
        "    conv3 = tf.keras.layers.Conv2D( 128 , kernel_size=( 3 , 3 ) , strides=1 )( conv3 )\n",
        "    conv3 = tf.keras.layers.LeakyReLU()( conv3 )\n",
        "\n",
        "    bottleneck = tf.keras.layers.Conv2D( 128 , kernel_size=( 3 , 3 ) , strides=1 , activation='tanh' , padding='same' )( conv3 )\n",
        "\n",
        "    concat_1 = tf.keras.layers.Concatenate()( [ bottleneck , conv3 ] )\n",
        "    conv_up_3 = tf.keras.layers.Conv2DTranspose( 128 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' )( concat_1 )\n",
        "    conv_up_3 = tf.keras.layers.Conv2DTranspose( 128 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' )( conv_up_3 )\n",
        "    conv_up_3 = tf.keras.layers.Conv2DTranspose( 64 , kernel_size=( 5 , 5 ) , strides=1 , activation='relu' )( conv_up_3 )\n",
        "\n",
        "    concat_2 = tf.keras.layers.Concatenate()( [ conv_up_3 , conv2 ] )\n",
        "    conv_up_2 = tf.keras.layers.Conv2DTranspose( 64 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' )( concat_2 )\n",
        "    conv_up_2 = tf.keras.layers.Conv2DTranspose( 64 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' )( conv_up_2 )\n",
        "    conv_up_2 = tf.keras.layers.Conv2DTranspose( 32 , kernel_size=( 5 , 5 ) , strides=1 , activation='relu' )( conv_up_2 )\n",
        "\n",
        "    concat_3 = tf.keras.layers.Concatenate()( [ conv_up_2 , conv1 ] )\n",
        "    conv_up_1 = tf.keras.layers.Conv2DTranspose( 32 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu')( concat_3 )\n",
        "    conv_up_1 = tf.keras.layers.Conv2DTranspose( 32 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu')( conv_up_1 )\n",
        "    conv_up_1 = tf.keras.layers.Conv2DTranspose( 3 , kernel_size=( 5 , 5 ) , strides=1 , activation='relu')( conv_up_1 )\n",
        "\n",
        "    model = tf.keras.models.Model( inputs , conv_up_1 )\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4I2zbQGkfJs-",
        "outputId": "56ff3cbd-1e26-4ec2-d73b-37f2b5eb5170"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 120, 120, 3)]     0         \n",
            "                                                                 \n",
            " conv2d_20 (Conv2D)          (None, 57, 57, 32)        4736      \n",
            "                                                                 \n",
            " conv2d_21 (Conv2D)          (None, 27, 27, 64)        51264     \n",
            "                                                                 \n",
            " conv2d_22 (Conv2D)          (None, 13, 13, 128)       73856     \n",
            "                                                                 \n",
            " conv2d_23 (Conv2D)          (None, 11, 11, 256)       295168    \n",
            "                                                                 \n",
            " conv2d_24 (Conv2D)          (None, 11, 11, 1)         2305      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 427,329\n",
            "Trainable params: 427,329\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "\n",
        "def get_discriminator_model():\n",
        "    layers = [\n",
        "        Conv2D(32, kernel_size=(7, 7), strides=2, activation='relu', input_shape=(120, 120, 3)),\n",
        "        Conv2D(64, kernel_size=(5, 5), strides=2, activation='relu'),\n",
        "        Conv2D(128, kernel_size=(3, 3), strides=2, activation='relu'),\n",
        "        Conv2D(256, kernel_size=(3, 3), strides=1, activation='relu'),\n",
        "    ]\n",
        "\n",
        "    inputs = tf.keras.Input(shape=(120, 120, 3))\n",
        "    x = inputs\n",
        "    for layer in layers:\n",
        "        x = layer(x)\n",
        "\n",
        "    # PatchGAN discriminator output\n",
        "    patch_output = Conv2D(1, kernel_size=(3, 3), strides=1, activation='sigmoid', padding='same')(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=patch_output)\n",
        "    return model\n",
        "\n",
        "# Example usage\n",
        "discriminator = get_discriminator_model()\n",
        "discriminator.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KI_88D3qfL3V"
      },
      "outputs": [],
      "source": [
        "cross_entropy = tf.keras.losses.BinaryCrossentropy()\n",
        "mse = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output) - tf.random.uniform( shape=real_output.shape , maxval=0.1 ) , real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output) + tf.random.uniform( shape=fake_output.shape , maxval=0.1  ) , fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "def generator_loss(fake_output , real_y):\n",
        "    real_y = tf.cast( real_y , 'float32' )\n",
        "    return mse( fake_output , real_y )\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam( 0.0005 )\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam( 0.0005 )\n",
        "\n",
        "generator = get_generator_model()\n",
        "discriminator = get_discriminator_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9Tt-ovsfOTd"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step( input_x , real_y ):\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        # Generate an image -> G( x )\n",
        "        generated_images = generator( input_x , training=True)\n",
        "        # Probability that the given image is real -> D( x )\n",
        "        real_output = discriminator( real_y, training=True)\n",
        "        # Probability that the given image is the one generated -> D( G( x ) )\n",
        "        generated_output = discriminator(generated_images, training=True)\n",
        "\n",
        "        # L2 Loss -> || y - G(x) ||^2\n",
        "        gen_loss = generator_loss( generated_images , real_y )\n",
        "        # Log loss for the discriminator\n",
        "        disc_loss = discriminator_loss( real_output, generated_output )\n",
        "\n",
        "        losses[\"D\"].append(disc_loss.numpy())\n",
        "        losses[\"G\"].append(gen_loss.numpy())\n",
        "    #tf.keras.backend.print_tensor( tf.keras.backend.mean( gen_loss ) )\n",
        "    #tf.keras.backend.print_tensor( gen_loss + disc_loss )\n",
        "\n",
        "    # Compute the gradients\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    # Optimize with Adam\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "generator.compile(\n",
        "    optimizer=generator_optimizer,\n",
        "    loss=generator_loss,\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "discriminator.compile(\n",
        "    optimizer=discriminator_optimizer,\n",
        "    loss=discriminator_loss,\n",
        "    metrics=['accuracy']\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AG783eo1fRB6"
      },
      "outputs": [],
      "source": [
        "def plot_loss(losses):\n",
        "    \"\"\"\n",
        "    @losses.keys():\n",
        "        0: loss\n",
        "        1: accuracy\n",
        "    \"\"\"\n",
        "    g_loss = []\n",
        "    d_loss = []\n",
        "\n",
        "    count = 0\n",
        "    for i in losses['D']:\n",
        "      count += 1\n",
        "      if(count == 36):\n",
        "        d_loss.append(i)\n",
        "        count = 0\n",
        "\n",
        "    count = 0\n",
        "    for i in losses['G']:\n",
        "      count += 1\n",
        "      if(count == 36):\n",
        "        g_loss.append(i)\n",
        "        count = 0\n",
        "\n",
        "    plt.figure(figsize=(10,8))\n",
        "    plt.plot(d_loss, label=\"Discriminator loss\")\n",
        "    plt.plot(g_loss, label=\"Generator loss\")\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqxS8a0SfUac",
        "outputId": "5d21f222-be2c-4b45-ca35-bc4449daeabe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running epoch :  0\n"
          ]
        }
      ],
      "source": [
        "# Please have a look at the Notebook in pdf form that was train on 150 epoc.\n",
        "num_epochs = 200\n",
        "losses = {\"D\":[], \"G\":[]}\n",
        "for e in range( num_epochs ):\n",
        "    print(\"Running epoch : \", e )\n",
        "    for ( x , y ) in dataset:\n",
        "        # Here ( x , y ) represents a batch from our training dataset.\n",
        "        # print( x.shape )\n",
        "        train_step( x , y )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tu3KBaAdfaYi"
      },
      "outputs": [],
      "source": [
        "y = generator( test_x[0 : ] ).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9a0gKxsXfdl4"
      },
      "outputs": [],
      "source": [
        "for i in range(11, 33, 7):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "\n",
        "    or_image = plt.subplot(3, 3, 1)\n",
        "    or_image.set_title('Grayscale Input', fontsize=16)\n",
        "    plt.imshow(test_x[i].reshape((120, 120)), cmap='gray')\n",
        "\n",
        "    in_image = plt.subplot(3, 3, 2)\n",
        "    image = cv.resize((y[i] * 255).astype('uint8'), (1024, 1024))\n",
        "    in_image.set_title('Colorized Output', fontsize=16)\n",
        "    plt.imshow(cv.cvtColor(image, cv.COLOR_LAB2RGB))\n",
        "\n",
        "    ou_image = plt.subplot(3, 3, 3)\n",
        "    image = cv.resize((test_y[i] * 255).astype('uint8'), (1024, 1024))\n",
        "    ou_image.set_title('Ground Truth', fontsize=16)\n",
        "    plt.imshow(cv.cvtColor(image, cv.COLOR_LAB2RGB))\n",
        "\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ux6jAd58fg3t"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4ZoynrkGRxe7aj/rx7gAb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}